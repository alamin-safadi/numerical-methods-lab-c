ğŸ“ˆ Least Squares Polynomial Curve Fitting (C)
ğŸ“Œ Description

This program implements the Least Squares Method to fit a polynomial curve of a given degree to a set of observed data points.
It finds the polynomial that minimizes the sum of squared errors between the actual data points and the predicted values.

The system of equations generated from the least squares formulation is solved using Gaussian Elimination with Partial Pivoting, ensuring numerical stability.

ğŸ¯ Objective

To determine the best-fitting polynomial of degree m:

ğ‘¦
=
ğ‘
0
+
ğ‘
1
ğ‘¥
+
ğ‘
2
ğ‘¥
2
+
â‹¯
+
ğ‘
ğ‘š
ğ‘¥
ğ‘š
y=a
0
	â€‹

+a
1
	â€‹

x+a
2
	â€‹

x
2
+â‹¯+a
m
	â€‹

x
m

such that the sum of squared residuals is minimized:

ğ‘†
=
âˆ‘
ğ‘–
=
1
ğ‘›
[
ğ‘¦
ğ‘–
âˆ’
(
ğ‘
0
+
ğ‘
1
ğ‘¥
ğ‘–
+
ğ‘
2
ğ‘¥
ğ‘–
2
+
â‹¯
+
ğ‘
ğ‘š
ğ‘¥
ğ‘–
ğ‘š
)
]
2
S=
i=1
âˆ‘
n
	â€‹

[y
i
	â€‹

âˆ’(a
0
	â€‹

+a
1
	â€‹

x
i
	â€‹

+a
2
	â€‹

x
i
2
	â€‹

+â‹¯+a
m
	â€‹

x
i
m
	â€‹

)]
2
ğŸ§  Mathematical Foundation
ğŸ”¹ Normal Equations

Minimizing the error function leads to (m+1) normal equations:

âˆ‘
ğ‘¦
ğ‘–
=
ğ‘
0
ğ‘›
+
ğ‘
1
âˆ‘
ğ‘¥
ğ‘–
+
ğ‘
2
âˆ‘
ğ‘¥
ğ‘–
2
+
â‹¯
+
ğ‘
ğ‘š
âˆ‘
ğ‘¥
ğ‘–
ğ‘š
âˆ‘y
i
	â€‹

=a
0
	â€‹

n+a
1
	â€‹

âˆ‘x
i
	â€‹

+a
2
	â€‹

âˆ‘x
i
2
	â€‹

+â‹¯+a
m
	â€‹

âˆ‘x
i
m
	â€‹

âˆ‘
ğ‘¥
ğ‘–
ğ‘¦
ğ‘–
=
ğ‘
0
âˆ‘
ğ‘¥
ğ‘–
+
ğ‘
1
âˆ‘
ğ‘¥
ğ‘–
2
+
â‹¯
+
ğ‘
ğ‘š
âˆ‘
ğ‘¥
ğ‘–
ğ‘š
+
1
âˆ‘x
i
	â€‹

y
i
	â€‹

=a
0
	â€‹

âˆ‘x
i
	â€‹

+a
1
	â€‹

âˆ‘x
i
2
	â€‹

+â‹¯+a
m
	â€‹

âˆ‘x
i
m+1
	â€‹

â‹®
â‹®
âˆ‘
ğ‘¥
ğ‘–
ğ‘š
ğ‘¦
ğ‘–
=
ğ‘
0
âˆ‘
ğ‘¥
ğ‘–
ğ‘š
+
ğ‘
1
âˆ‘
ğ‘¥
ğ‘–
ğ‘š
+
1
+
â‹¯
+
ğ‘
ğ‘š
âˆ‘
ğ‘¥
ğ‘–
2
ğ‘š
âˆ‘x
i
m
	â€‹

y
i
	â€‹

=a
0
	â€‹

âˆ‘x
i
m
	â€‹

+a
1
	â€‹

âˆ‘x
i
m+1
	â€‹

+â‹¯+a
m
	â€‹

âˆ‘x
i
2m
	â€‹


These equations are solved to obtain the coefficients 
ğ‘
0
,
ğ‘
1
,
â€¦
,
ğ‘
ğ‘š
a
0
	â€‹

,a
1
	â€‹

,â€¦,a
m
	â€‹

.

âš™ï¸ Algorithm Overview
Step 1: Input Phase

Input number of data points n

Input polynomial degree m

Condition:

1
â‰¤
ğ‘š
<
ğ‘›
â‰¤
MAX_POINTS
1â‰¤m<nâ‰¤MAX_POINTS

Input data points 
(
ğ‘¥
ğ‘–
,
ğ‘¦
ğ‘–
)
(x
i
	â€‹

,y
i
	â€‹

)

Check for duplicate 
ğ‘¥
x-values (warning if found)

Step 2: Summation Calculation

Initialize:

sum_x[k] = Î£ x^k for 
ğ‘˜
=
0
k=0 to 
2
ğ‘š
2m

sum_xy[k] = Î£ x^k y for 
ğ‘˜
=
0
k=0 to 
ğ‘š
m

For each data point:

Compute successive powers of 
ğ‘¥
x

Accumulate required sums

Step 3: Construct Normal Equation Matrix

Create augmented matrix A of size 
(
ğ‘š
+
1
)
Ã—
(
ğ‘š
+
2
)
(m+1)Ã—(m+2):

ğ´
[
ğ‘–
]
[
ğ‘—
]
=
âˆ‘
ğ‘¥
ğ‘–
+
ğ‘—
A[i][j]=âˆ‘x
i+j
ğ´
[
ğ‘–
]
[
ğ‘š
+
1
]
=
âˆ‘
ğ‘¥
ğ‘–
ğ‘¦
A[i][m+1]=âˆ‘x
i
y

This represents the linear system:

ğ´
â‹…
ğ‘‹
=
ğµ
Aâ‹…X=B
Step 4: Solve Linear System (Gaussian Elimination)

Use Gaussian Elimination with Partial Pivoting:

Forward elimination

Row swapping for numerical stability

Back substitution

This yields polynomial coefficients:

ğ‘‹
=
[
ğ‘
0
,
ğ‘
1
,
â€¦
,
ğ‘
ğ‘š
]
X=[a
0
	â€‹

,a
1
	â€‹

,â€¦,a
m
	â€‹

]
Step 5: Goodness of Fit Evaluation

Compute mean of observed values:

ğ‘¦
Ë‰
=
1
ğ‘›
âˆ‘
ğ‘¦
ğ‘–
y
Ë‰
	â€‹

=
n
1
	â€‹

âˆ‘y
i
	â€‹


Total sum of squares:

ğ‘†
ğ‘†
ğ‘¡
ğ‘œ
ğ‘¡
ğ‘
ğ‘™
=
âˆ‘
(
ğ‘¦
ğ‘–
âˆ’
ğ‘¦
Ë‰
)
2
SS
total
	â€‹

=âˆ‘(y
i
	â€‹

âˆ’
y
Ë‰
	â€‹

)
2

Residual sum of squares:

ğ‘†
ğ‘†
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘–
ğ‘‘
ğ‘¢
ğ‘
ğ‘™
=
âˆ‘
(
ğ‘¦
ğ‘–
âˆ’
ğ‘¦
^
ğ‘–
)
2
SS
residual
	â€‹

=âˆ‘(y
i
	â€‹

âˆ’
y
^
	â€‹

i
	â€‹

)
2

Coefficient of determination:

ğ‘…
2
=
1
âˆ’
ğ‘†
ğ‘†
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘–
ğ‘‘
ğ‘¢
ğ‘
ğ‘™
ğ‘†
ğ‘†
ğ‘¡
ğ‘œ
ğ‘¡
ğ‘
ğ‘™
R
2
=1âˆ’
SS
total
	â€‹

SS
residual
	â€‹

	â€‹


Mean Absolute Error (MAE)

Step 6: Output Results

Display fitted polynomial equation

Display coefficients

Display RÂ² value and error statistics

Display table of actual vs predicted values

ğŸ” Gaussian Elimination Algorithm
Forward Elimination (with Partial Pivoting)

Select pivot row with maximum absolute value

Swap rows if necessary

Eliminate variables below pivot

Back Substitution

Solve last equation

Substitute upward to find remaining unknowns

â± Time Complexity
Step	Complexity
Input	O(n)
Summation	O(nÂ·m)
Matrix Construction	O(mÂ²)
Gaussian Elimination	O(mÂ³)
Prediction & Error	O(nÂ·m)
Total	O(nÂ·m + mÂ³)
ğŸ’¾ Memory Requirements
Component	Space
Data points	O(n)
Sum arrays	O(m)
Matrix	O(mÂ²)
Coefficients	O(m)
ğŸ“Œ Applications

Scientific data fitting

Engineering curve approximation

Trend analysis & forecasting

Polynomial regression (Machine Learning)

Signal smoothing

âš ï¸ Limitations

Sensitive to outliers

High-degree polynomials may overfit

Numerical instability for large degree

Equal weighting of all data points

ğŸ›  Compilation & Execution
gcc least_squares.c -o least_squares -lm
./least_squares

âœ… Key Features

Polynomial fitting up to degree 10

Gaussian elimination with pivoting

RÂ² goodness-of-fit calculation

Mean absolute error calculation

Detailed, educational output

ğŸ“š Educational Value

This program demonstrates:

Least Squares theory

Normal equations

Matrix formulation

Gaussian elimination

Numerical stability techniques
