# ğŸ“ˆ Least Squares Polynomial Curve Fitting (C)

## ğŸ“Œ Description
This program implements the **Least Squares Method** to fit a polynomial curve of a given degree to a set of observed data points.  
It determines the polynomial that **minimizes the sum of squared errors** between the actual data values and the predicted values.

The system of equations obtained from the least squares formulation is solved using **Gaussian Elimination with Partial Pivoting**, which improves numerical stability.

---

## ğŸ¯ Objective
To determine the best-fitting polynomial of degree **m**:

y = aâ‚€ + aâ‚x + aâ‚‚xÂ² + Â·Â·Â· + aâ‚˜xáµ

such that the **sum of squared residuals** is minimized:

S = Î£ [ yáµ¢ âˆ’ (aâ‚€ + aâ‚xáµ¢ + aâ‚‚xáµ¢Â² + Â·Â·Â· + aâ‚˜xáµ¢áµ) ]Â²

---

## ğŸ§  Mathematical Foundation

### ğŸ”¹ Normal Equations
Minimizing the error function leads to **(m + 1) normal equations**:

Î£ yáµ¢ = aâ‚€ n + aâ‚ Î£ xáµ¢ + aâ‚‚ Î£ xáµ¢Â² + Â·Â·Â· + aâ‚˜ Î£ xáµ¢áµ  

Î£ xáµ¢ yáµ¢ = aâ‚€ Î£ xáµ¢ + aâ‚ Î£ xáµ¢Â² + Â·Â·Â· + aâ‚˜ Î£ xáµ¢áµâºÂ¹  

â‹®  

Î£ xáµ¢áµ yáµ¢ = aâ‚€ Î£ xáµ¢áµ + aâ‚ Î£ xáµ¢áµâºÂ¹ + Â·Â·Â· + aâ‚˜ Î£ xáµ¢Â²áµ  

Solving these equations gives the polynomial coefficients  
aâ‚€, aâ‚, aâ‚‚, â€¦ , aâ‚˜.

---

## âš™ï¸ Algorithm Overview

### Step 1: Input Phase
1. Input number of data points **n**
2. Input polynomial degree **m**
   - Condition:  
     1 â‰¤ m < n â‰¤ MAX_POINTS
3. Input data points (xáµ¢, yáµ¢)
4. Check for duplicate x-values (warning if found)

---

### Step 2: Summation Calculation
Initialize:
- sum_x[k] = Î£ xáµ  for k = 0 to 2m  
- sum_xy[k] = Î£ xáµ y for k = 0 to m  

For each data point:
- Compute successive powers of x
- Accumulate required summations

---

### Step 3: Construct Normal Equation Matrix
Create augmented matrix **A** of size (m + 1) Ã— (m + 2):

A[i][j] = Î£ xâ½â±âºÊ²â¾  
A[i][m+1] = Î£ xâ± y  

This represents the linear system:

A Â· X = B

---

### Step 4: Solve Linear System
Solve the system using **Gaussian Elimination with Partial Pivoting**:
- Forward elimination
- Row swapping for numerical stability
- Back substitution

This yields the coefficient vector:

X = [ aâ‚€, aâ‚, â€¦ , aâ‚˜ ]

---

### Step 5: Goodness of Fit Evaluation
1. Mean of observed values:
   yÌ„ = (1 / n) Î£ yáµ¢
2. Total sum of squares:
   SS_total = Î£ (yáµ¢ âˆ’ yÌ„)Â²
3. Residual sum of squares:
   SS_residual = Î£ (yáµ¢ âˆ’ Å·áµ¢)Â²
4. Coefficient of determination:
   RÂ² = 1 âˆ’ (SS_residual / SS_total)
5. Mean Absolute Error (MAE)

---

### Step 6: Output Results
- Display fitted polynomial equation
- Display coefficients aâ‚€ to aâ‚˜
- Display RÂ² and error statistics
- Display table of actual vs predicted values

---

## ğŸ” Gaussian Elimination Algorithm

### Forward Elimination (with Partial Pivoting)
1. Select pivot row with maximum absolute value
2. Swap rows if required
3. Eliminate variables below the pivot

### Back Substitution
1. Solve the last equation
2. Substitute upward to find remaining unknowns

---

## â± Time Complexity

| Step | Complexity |
|-----|------------|
| Input | O(n) |
| Summation | O(nÂ·m) |
| Matrix construction | O(mÂ²) |
| Gaussian elimination | O(mÂ³) |
| Prediction & error | O(nÂ·m) |
| **Total** | **O(nÂ·m + mÂ³)** |

---

## ğŸ’¾ Memory Requirements

| Component | Space |
|----------|-------|
| Data points | O(n) |
| Sum arrays | O(m) |
| Matrix | O(mÂ²) |
| Coefficients | O(m) |

---

## ğŸ“Œ Applications
- Scientific data fitting
- Engineering curve approximation
- Trend analysis and forecasting
- Polynomial regression (Machine Learning)
- Signal smoothing

---

## âš ï¸ Limitations
- Sensitive to outliers
- High-degree polynomials may overfit
- Numerical instability for large degree
- Equal weighting of all data points

---

## ğŸ›  Compilation & Execution
```bash
gcc least_squares.c -o least_squares -lm
./least_squares
âœ… Key Features
Polynomial fitting up to degree 10

Gaussian elimination with partial pivoting

RÂ² goodness-of-fit calculation

Mean absolute error computation

Clear, educational output

ğŸ“š Educational Value
This program demonstrates:

Least Squares theory

Normal equation formulation

Matrix-based solution

Gaussian elimination

Numerical stability techniques

Author: Md Alamin
Course: Numerical Methods Lab
Purpose: Academic and learning use
